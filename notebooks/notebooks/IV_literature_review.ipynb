{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59d8dbe-b975-49ee-9b58-69eb35f35b9f",
   "metadata": {},
   "source": [
    "# IV. Applying the Framework to Recent Literature\n",
    "\n",
    "Using our framework, we now assess if there is currently a common ground on corpus creation in research. Thus, we systematically review 44 academic top tier papers to collect and analyze data on the measures from Section III.\n",
    "\n",
    "We distill recent top tier papers on binary firmware vulnerability research that underwent rigorous peer reviews to ensure the data is based on state of the art scientific practices. Collection started by downloading all papers from CCS, NDSS, SP, and USENIX Security. These are the only four cybersecurity conferences with the highest rating of A* in the CORE2023 ranking. For actuality, we considered work published between 2013 and 2023. We skimmed the abstracts and removed all papers that do not focus on vulnerability research. The resulting set contained 263 papers. We then screened their full-text for the keyword Firmware and removed items without a match, as they likely do not explore this research branch. 65 papers remained. Assuming that high-quality research references other high-quality research, we read the related work sections for referenced work between 2013 and 2023 that focuses on Firmware security as well. Thus, we left the CORE2023 ranking and added 32 referenced papers from workshops and conferences like IoT SP, ACSAC,\n",
    "NDSS BAR, and RAID. We skimmed the evaluation methods of the grown set of 97 papers and discarded all papers that do not create or use a firmware corpus. The final set, listed in Table I, has 44 papers from 10 workshops and conferences.\n",
    "\n",
    "We read every paper and collected all necessary data to apply our framework: We investigated the fulfillment of our requirements using the 16 measures, noted the analysis methods each paper uses, and estimated their scalability. Scalability was estimated by used analysis methods and evaluation results. If there were multiple corpora, we inspected them separately, but only considered corpora with real-world samples. We inspected shared artifacts to find information on the measures if they were not explicitly mentioned. We marked when a measure is not applicable to specific paper scenarios. IoTFuzzer, e.g., uses HIL fuzzing and does not\n",
    "need unpacking. We distinguish complete, partial, and missing documentation per measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6cf02-2918-472d-ab14-5e67f570cea6",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "Below you will find preparatory stuff such as imports and constant definitions for use down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff187f-5616-4240-84c6-759ee1a0b5df",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace560d9-51b4-40c8-9d4e-6bd40f9376a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import ScalarFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecdd92-4752-4882-b42e-ad81ed442e82",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e54495-19f3-4109-aa0e-ba09bad5888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LITERATURE_OVERVIEW: Path = Path(\"../public_data/literature_overview.csv\")\n",
    "LITERATURE_RESULTS: Path = Path(\"../public_data/literature_results.csv\")\n",
    "FIGURE_DEST: Path = Path(\"../figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3300bb-86e5-4b03-b14e-218dbe11dfcb",
   "metadata": {},
   "source": [
    "### Matplotlib Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e8f2d-b1e7-4ed1-b48a-8fb6b9884573",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Times\"], \"size\": 15})\n",
    "rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845dc947-2838-43dd-8c79-0f43b318b89d",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6979f20f-b65c-4440-981c-15a550e570d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    return pd.read_csv(LITERATURE_OVERVIEW), pd.read_csv(LITERATURE_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaac6a-3dc7-45d5-8624-40971180417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overview: pd.DataFrame\n",
    "df_results: pd.DataFrame\n",
    "\n",
    "df_overview, df_results = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b463a0-ad49-495f-a2b8-70c5df589495",
   "metadata": {},
   "source": [
    "## Peek into Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3b581-deae-40e2-9ec4-d6ea577b2b90",
   "metadata": {},
   "source": [
    "### Overview of Reviewed Research Papers (Table I)\n",
    "\n",
    "Provide an overview of the reviewed research papers.\n",
    "\n",
    "#### Legend\n",
    "\n",
    "* Scalability\n",
    "  * `Y` = Yes\n",
    "  * `N` = No\n",
    "* Method\n",
    "  * `CS` = Code Similarity\n",
    "  * `E` = Emulation\n",
    "  * `F` = Fuzzing\n",
    "  * `FA` = Flow Analysis\n",
    "  * `HIL` = Hardware-In-the-Loop\n",
    "  * `ML` = Machine Learning\n",
    "  * `P` = Pattern\n",
    "  * `SE` = Symbolic Execution\n",
    "  * `;` = Separator\n",
    "* Type\n",
    "  * `S` = Static\n",
    "  * `D` = Dynamic\n",
    "  * `H` = Hybrid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfed0e-c031-45a4-a5bd-beb15b5e3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468c7c0-4653-4eb8-a402-54d6b68ce923",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Corpus Creation Practices in Top Tier Research from 2013 to 2023: Collected Data on the Measures for Scientifically Sound Firmware Corpora (Table II)\n",
    "\n",
    "Visualize the 704 data points we collected across all 44 research papers.\n",
    "\n",
    "#### Legend\n",
    "\n",
    "* Symbols\n",
    "  * `Y` = Documented/Proof of Presence in Data\n",
    "  * `N` = Not Documented/Proof of Absence in Data\n",
    "  * `U` = Partially Documented/Missing Data to Proof Absence or Presence\n",
    "  * `NaN` = Not Applicable in Paper Scenario\n",
    "  * `;` = Separator for Multiple Methods/Corpora/Data Points\n",
    "* Acquisition\n",
    "  * `S`= Web-Scraping\n",
    "  * `M` = Manual Collection\n",
    "  * `R` = Samples from Related Work\n",
    "* Firmware Types\n",
    "  * See Sec. II-A in Paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e551-2978-4011-b3e1-61a154a30ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66312f4-f3a0-4b18-bbb5-6d3844641de0",
   "metadata": {},
   "source": [
    "## Merge Overview and Results to Large Table for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6bc4d-912e-4336-886f-ea288cdb5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(df_left: pd.DataFrame, df_right: pd.DataFrame) -> pd.DataFrame:\n",
    "    df: df.DataFrame = pd.merge(df_left, df_right, left_index=True, right_index=True, how=\"left\")\n",
    "    df.drop(columns=\"Paper_y\", inplace=True)\n",
    "    df.rename(columns={\"Paper_x\": \"Paper\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aa266-ad1f-4152-b0a9-efbb558565c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = merge_dataframes(df_overview, df_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfd2c0-f811-4372-b03e-eed182b9eb9b",
   "metadata": {},
   "source": [
    "## IV-B. General Statistics & Result Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c52d84-8137-4835-9b0b-253d7deb2693",
   "metadata": {},
   "source": [
    "### Statement: Each year from 2013 to 2023 is represented, with rising quantities until 2021. Few included papers were published in 2022 and 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9055aca-845a-48d1-b724-235ec80aaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Paper\", \"Year\"]].groupby(\"Year\").count().rename(columns={\"Paper\": \"Papers [#]\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d37944-2217-4e2d-8039-92199140a617",
   "metadata": {},
   "source": [
    "### Statement: The four most represented conferences are USENIX (13 papers), CCS (9), NDSS (8), and SP (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063fb36-dd76-4147-8466-b8bc1594349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Paper\", \"Conference\"]].groupby(\"Conference\").count().sort_values([\"Paper\"], ascending=False).rename(\n",
    "    columns={\"Paper\": \"Papers [#]\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc69cc-7b9d-4867-90c3-cbacaa84ad88",
   "metadata": {},
   "source": [
    "### Statement: 22 papers describe an entirely static (S), seven a dynamic (D), and 15 a hybrid (H) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99b63c-0c49-4e11-a075-5e296e4a998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Paper\", \"Type\"]].groupby(\"Type\").count().sort_values([\"Paper\"], ascending=False).rename(\n",
    "    columns={\"Paper\": \"Papers [#]\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b3021-2217-4cf8-8f87-13da59380452",
   "metadata": {},
   "source": [
    "### Statement: 28 methods are rated as scalable and seven as unscalable. For nine papers, there is uncertainty regarding scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f20abb-b87f-43f0-9923-5041c3b7386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Paper\", \"Scalable\"]].groupby(\"Scalable\").count().sort_values([\"Paper\"], ascending=False).rename(\n",
    "    columns={\"Paper\": \"Papers [#]\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880a762-b273-4e8a-94f8-79e83bee6d21",
   "metadata": {},
   "source": [
    "As for the unscalable ones. Which use HIL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d102834-9507-4073-9eff-35b5bcf9137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"Scalable\"] == \"N\") & df[\"Method\"].str.contains(\"HIL\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba2ab6-01ed-40d1-a8be-b46bf6ffdb4c",
   "metadata": {},
   "source": [
    "Answer: **All of them**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eda2cb-4584-4ca5-a38a-54673daffebe",
   "metadata": {},
   "source": [
    "## IV-C. Preliminary Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d760c4-23a8-48c7-8850-b321b9599165",
   "metadata": {},
   "source": [
    "### A paper's scenario dictates feasible quantities.\n",
    "G3 says that all numbers in Table II are relative to their paper scenario. Aspects like sample accessibility and scalability influence experiment setups. The data backs this claim: Table II \\[in the paper\\] reveals that 17 out of 44 papers use corpora between 373 and 33,000 packed samples. All of them use scalable methods according to Table I; the majority of them scrapes the accessible Type-I. Only one of the 17 papers includes the more specialized and harder to acquire Type-III, but does not target it exclusively.\n",
    "\n",
    "Vice versa, the other 27 papers use corpora of two to 49 packed samples. Out of these, about 70% either target Type-III or use HIL. Thus, low quantities must not indicate bad practice, as they can also reveal limits of feasibility in spite of best scien-\n",
    "tific efforts. This does not change the fact that few data points introduce statistical uncertainty, affecting representativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed5645-c808-4d00-b805-3bd9f70e48f9",
   "metadata": {},
   "source": [
    "#### => 17 out of 44 papers use corpora between 373 and 33,000 packed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9e350-0e3e-4cb7-83fe-ae680049bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_packed_data_row_to_numerics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_tmp: pd.DataFrame = df[[\"Paper\", \"Packed\"]].copy()\n",
    "    # 1. Firmup is vague with its packed samples, so it's value is \"U Packed\"... we cannot convert that to numerics\n",
    "    # 2. Some papers use multiple corpora, shown as semicolon-separated list in this dataset. The list is ordered left-to-right, always use the larger corpus numbers\n",
    "    df_tmp[\"Packed\"] = df_tmp[\"Packed\"].str.replace(r\"(U\\s)|(\\d+;)\", \"\", regex=True)\n",
    "    df_tmp[\"Packed\"] = pd.to_numeric(df_tmp[\"Packed\"])\n",
    "    return df_tmp\n",
    "\n",
    "\n",
    "df_packed_sizes: pd.DataFrame = convert_packed_data_row_to_numerics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd82b5-4e61-46f6-bc38-a69cebfeaa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 300 < df_packed_sizes[\"Packed\"]\n",
    "df_packed_sizes[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643fc79-c6cc-4b3f-8a54-2e1edda677c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Papers: {df_packed_sizes[query].count()['Paper']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892921a-94e4-4cdc-8f35-20eb54afa04d",
   "metadata": {},
   "source": [
    "#### All of the 17 papers use scalable methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1f353-9ae7-4b00-8d02-e404e035e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_packed_sizes = 300 < df_packed_sizes[\"Packed\"]\n",
    "query_is_scalable = df[\"Scalable\"] == \"Y\"\n",
    "\n",
    "\n",
    "df[query_packed_sizes & query_is_scalable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918679b6-8847-4cb3-825f-d1a603f9aa03",
   "metadata": {},
   "source": [
    "#### The majority of these 17 Papers scrape the accessible Type-I. Only one of the 17 papers includes the more specialized and harder to acquire Type-III, but does not target it exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bffd438-16b3-4fd9-80c0-7dc1c3ea2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_uses_scraping = df[\"Acquisition\"].str.contains(\"S\")\n",
    "\n",
    "df_scrape_fw_types = (\n",
    "    df[query_packed_sizes & query_is_scalable & query_uses_scraping][[\"Paper\", \"FW Types\"]].groupby(\"FW Types\").count()\n",
    ")\n",
    "\n",
    "df_scrape_fw_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509fefe-8e3c-443c-8f19-7f4fb7e85ad2",
   "metadata": {},
   "source": [
    "#### The other 27 papers use corpora of two to 49 packed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0ba7e-9ce9-453f-9c86-1e0150f0818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_low_packed = 300 >= df_packed_sizes[\"Packed\"]\n",
    "df_packed_sizes[query_low_packed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc7fca-7cde-4078-af57-a4abb8337ab6",
   "metadata": {},
   "source": [
    "#### Out of these (27 papers), about 70% either target Type-III or use HIL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271947ef-c27d-4082-a187-3efa4b710732",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_targets_type_iii = df[\"FW Types\"].str.contains(\"III\")\n",
    "query_uses_hil = df[\"Method\"].str.contains(\"HIL\")\n",
    "\n",
    "df[query_low_packed & (query_targets_type_iii | query_uses_hil)][\"Paper\"].count() / df[query_low_packed][\n",
    "    \"Paper\"\n",
    "].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef04813-cdf3-4508-8703-9b2af1d5cc71",
   "metadata": {},
   "source": [
    "### The measures are practicable and relevant\n",
    "We have created a practical framework that addresses scientifically relevant aspects of corpus creation. Thus, we proposed concrete measures to test the fulfillment of abstract requirements (cf. Section III-B). We assumed that these measures show real-world research relevance while being universally applicable.\n",
    "\n",
    "Table II holds 704 data points across the 16 proposed measures and 44 papers. We considered that measures may not be applicable. This is only true for 17 out of 704 data points (∼2%). The results further reveal that there are positive findings for each measure. Thus, all measures address corpus creation practices that find application in actual research. These observations let us conclude that our framework is applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fe289-ee27-41e8-86e9-1cb11fea99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.isna().count().count() / (df_results.shape[0] * (df_results.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba069e89-3052-433a-a2bf-77e25fad3186",
   "metadata": {},
   "source": [
    "## IV-D. Quantitative Result Analysis by Measure\n",
    "\n",
    "We perform quantitative analysis on the data in Table II to identify the cumulative measure performance across all papers and discuss current practices in research. We group data by measure and convert concrete numbers to the value . This way, we derive a consistent value set that removes the com plexity of numeric values such as sample quantities. We establish a comparison baseline using four discrete values: A paper documents the subject of a measure fully, partially, or not at all. The fourth is non-applicability. We calculated the fraction of fully (and partially) documented data points for a measure across all applicable papers. Results are unweighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447149b-b457-4b03-9bc3-22e0094ed761",
   "metadata": {},
   "source": [
    "### (Preparation) Calculate Absolute and Relative Statistics across all Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a735578-ad7e-4373-b208-f9c703f3169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    raw_stats = {\n",
    "        \"Measure\": [],\n",
    "        \"Total Applicable\": [],\n",
    "        \"Yes\": [],\n",
    "        \"Unclear\": [],\n",
    "        \"No\": [],\n",
    "    }\n",
    "\n",
    "    for name, column in [t for t in df.items()][6:]:\n",
    "        total = column.count()\n",
    "        n = column[column.str.count(pat=\"N\") > 0].count()\n",
    "        u = column[column.str.count(pat=\"U\") > 0].count()\n",
    "        y = total - n - u\n",
    "\n",
    "        raw_stats[\"Measure\"] += [name]\n",
    "        raw_stats[\"Total Applicable\"] += [total]\n",
    "        raw_stats[\"Yes\"] += [y]\n",
    "        raw_stats[\"No\"] += [n]\n",
    "        raw_stats[\"Unclear\"] += [u]\n",
    "\n",
    "    df_abs = pd.DataFrame(raw_stats)\n",
    "\n",
    "    # relative stats\n",
    "    df_rel = df_abs.copy()\n",
    "    df_rel.drop(columns=[\"Total Applicable\"], inplace=True)\n",
    "    for col in [\"Yes\", \"No\", \"Unclear\"]:\n",
    "        df_rel[col] /= df_abs[\"Total Applicable\"]\n",
    "\n",
    "    return df_abs, df_rel\n",
    "\n",
    "\n",
    "df_absolute: pd.DataFrame\n",
    "df_relative: pd.DataFrame\n",
    "\n",
    "\n",
    "df_absolute, df_relative = calculate_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367323da-292c-47f6-b52f-8b5efe9f5f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c81c8b-4748-4c1f-85e0-d3f83ca98933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb91270-2b8c-45a6-82ba-b2ddeb1e5c42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \\[Sample Quantities\\] All document packed sample quantities. Few do not give unpacked numbers.\n",
    "\n",
    "100% of the papers specify the quantity of packed samples in their corpora. Here, all except for one paper, FirmUp, give precise numbers. When samples got unpacked, the performance drops to 91%. Out of all papers, 81% give precise numbers. The remaining 10% originate from FirmUp and three other papers. The first gives approximate numbers, the others include the unpacker as system component but do not provide any clear number on this processing step. Three papers do not share unpacked quantities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450982f8-2899-4a1c-b4cd-55d58442fa17",
   "metadata": {},
   "source": [
    "Which papers share \"Packed\" quantities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a77f9-b1d1-426c-af52-6ec7453d45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Packed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f376042-db52-429f-b371-cd3441a53e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Packed\"].str.contains(\"U\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214a6b0-d18d-45c6-9154-983302cf36be",
   "metadata": {},
   "source": [
    "Which papers do not fully document \"Unpacked\" quantities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc58454-034b-41d1-a3bc-fe9e7bec5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Unpacked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f47cb-bfe2-4a95-b875-07d5a8b62a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Unpacked\"].astype(str).str.contains(r\"U|N\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0d2d2-f731-455b-96ec-33dcae042381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \\[Deduplication\\] 30% of the papers do not provide information on sample deduplication.\n",
    "\n",
    "Sample deduplication is importantto avoid skewness in analysis results due to, e.g., duplicate findings. We note that the performance on this measure is over-evaluated in terms of documentation awareness: The 70% already includes papers that share artifacts which helped us to determine if any deduplication took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016b42a-d077-41b4-9dcb-b821efb5feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Deduplication\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86fe61-3c16-445e-aa20-291674d9c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Deduplication\"].astype(str).str.contains(r\"U|N\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9fc7d-c56d-4d6a-b5b2-08ef8788860a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \\[Unpacking Process\\] 52% of the papers do not describe the unpacking process.\n",
    "\n",
    "If the unpacking process remains undocumented, it poses a barrier to any kind of replicability and, thus, result verification. 20 (52%) of all papers that are applicable to this measure do not document this critical process. 12 (32%) document it in detail, e.g, Greenhouse, FirmScope, and Karonte. 6 (16%) document it partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4d875-e32a-48ae-bfae-52ad90717fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Unpack Proc.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c54127-2d8b-41aa-8c58-070431488e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute[df_absolute[\"Measure\"] == \"Unpack Proc.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097569d-208a-4811-bcef-7c36219b28b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \\[Reasoning\\] 13 papers do not justify sample selection.\n",
    "\n",
    "It is useful for third parties to understand why a corpus contains certain samples, as such information gives insights on possible limitations and goals. It further helps to contextualize the work and interpret results. Possible reasons for sample selection could be, e.g., availability, required firmware properties like ISAs, or a device class of particular interest. 30% of papers do not give a reason, 18% give a reason that was not entirely\n",
    "comprehensible to us, and 52% justify comprehensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7a3be-d832-4e49-b6ea-3fa99806da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Reasoning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929b301-a444-4a52-90ef-c40a5c68724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute[df_absolute[\"Measure\"] == \"Reasoning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669a5f3-e82b-4c39-8b0a-43fda74bda6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### \\[Acquisition\\] 32% of the papers do not document acquisition.\n",
    "\n",
    "Sharing how samples were acquired points independent research into the direction of corpus replication, be it through scraping or manual firmware extraction. 14 out of 44 (32%) papers do not provide any information on this matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0baef-c51c-4c1c-8de5-4bd55239e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Acquisition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e355c-bfb0-4f1a-b730-9f469dfc8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute[df_absolute[\"Measure\"] == \"Acquisition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19008409-faf8-4f02-aea4-bd31b9cfe7f8",
   "metadata": {},
   "source": [
    "### \\[Known Vulnerabilities\\] 50% of the papers have no or incomplete documentation on the existence of known bugs in their corpora. \n",
    "\n",
    "The existence of known vulnerabilities in corpora helps to obtain verifiable evidence showing the fruitfulness of a new analysis method. If there are known bugs fitting to the paper scenario, it is a choice to include and/or\n",
    "search for them as benchmark or not. 21 out of 42 papers fully document the existence of such ground truth (50%). DTaint, e.g., rediscovers six verifiable CVEs in their corpus. Nine papers give partial information on this subject (21%). For instance, VulSeeker searches for CVE-2015- 1791, but does not provide information on which samples in the corpus are affected. Experiments with consistent results on other CVEs are mentioned, but not further explained. The remaining twelve papers do not mention ground truth (29%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa74a52-4f83-4993-ab13-52ecd71ef820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative[df_relative[\"Measure\"] == \"Vulnerabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d33ee-fd69-48aa-9902-c257661ec28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute[df_absolute[\"Measure\"] == \"Vulnerabilities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a3be4-a987-4ca8-93a4-9e8c987f4e51",
   "metadata": {},
   "source": [
    "### \\[File & Temporal Properties\\] Release dates, versions, source links, and hashes are rarely documented.\n",
    "\n",
    "Considering temporal properties that could help to estimate relevance, only four out of 44 papers report firmware release dates (4%) and 15 (34%) report firmware versions in their corpora. For the latter, there are four more papers with partial documentation: BootStomp, e.g., reports experiments on an older and newer bootloader version by Qualcomm, but does not name the identifiers. File properties beneficial to replicability are also rarely documented: 15 out of 44 papers share download links or device acquisition (34%). If such links become invalid, readers can fall back to file hashes to find alternative sources. Three papers provide an incomplete sample list, e.g., FirmSolo, who use the fully documented FIRMADYNE corpus but then add 50 samples of unknown origin. Hashes are available in seven out of 44 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee309e5-20b8-4631-886e-59ba90c2bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative.iloc[7:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aeda7e-9def-40ee-92bb-eba70d71e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute.iloc[7:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f53d20-5a1a-497b-b94d-c7ed9804f35a",
   "metadata": {},
   "source": [
    "### \\[Device Properties\\] All papers discuss corpus composition regarding heterogeneous device properties.\n",
    "\n",
    "In all papers, there is full or partial information on the device properties Manufacturer, Model, Device Class, ISA, and Firmware Type. This is a positive result as it shows that all papers provide insights on corpus heterogeneity. Yet, between 25% and 34% of papers only provide partial information. These can be grouped into two classes: First, there are papers that bulk download firmware images using scrapers but do not collect meta data, e.g., Costin et al.. Second, some papers give incomplete information on these properties. FirmUp, e.g., lists example manufacturers, but not all of them. In both cases, some device properties remain unknown, which makes it harder to assess corpus composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c545cdd-d6af-4168-b41e-7d64b6dc2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relative.iloc[11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8deb046-f422-410c-9fe5-af05b1f1c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_absolute.iloc[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf9a3d-ecba-4d6b-820e-2ddad2f1ff12",
   "metadata": {},
   "source": [
    "### Figure 4: Aggregated results of all collected data points for each measure in Table II. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44a07b-4ab3-42a0-80cf-6c7eee4520f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_four(df_relative: pd.DataFrame):\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    df_relative.rename(\n",
    "        columns={\"Yes\": \"Documented\", \"No\": \"Undocumented\", \"Unclear\": \"Partially Documented\"}, inplace=True\n",
    "    )\n",
    "    ax = df_relative.iloc[::-1].plot(\n",
    "        kind=\"barh\",\n",
    "        stacked=True,\n",
    "        x=\"Measure\",\n",
    "        y=[\"Documented\", \"Partially Documented\", \"Undocumented\"],\n",
    "        grid=True,\n",
    "        color=[palette[0], palette[1], palette[-3]],\n",
    "        figsize=(8, 7),\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.set_xticks(np.arange(0.0, 1.1, 0.1))\n",
    "    ax.set_xlabel(\"Fraction\")\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.legend(ncols=3, bbox_to_anchor=(0.975, 1.1))\n",
    "    ax.set_yticklabels(\n",
    "        [\n",
    "            \"Firmware Types\",\n",
    "            \"ISAs\",\n",
    "            \"Device Classes\",\n",
    "            \"Models\",\n",
    "            \"Manufacturer\",\n",
    "            \"Hashes\",\n",
    "            \"Links\",\n",
    "            \"Versions\",\n",
    "            \"Release Dates\",\n",
    "            \"Known Vulnerabilities\",\n",
    "            \"Acquisition\",\n",
    "            \"Reasoning\",\n",
    "            \"Unpack Process\",\n",
    "            \"Deduplication\",\n",
    "            \"Unpacked Samples\",\n",
    "            \"Packed Samples\",\n",
    "        ]\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURE_DEST / \"f4_relative_degree_of_measure_documentation_across_papers.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_figure_four(df_relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269ea82-007a-455f-af02-32ac73305659",
   "metadata": {},
   "source": [
    "## IV-F. Are Current Practices Meeting our Requirements?\n",
    "\n",
    "Figure 5 shows that researchers put significant effort into corpus creation. Yet, there is room for improvement: They could include more meta data that helps with replicability and representativeness (R4). One should provide release dates, version numbers, download links, and file hashes (R2, R4). Also, there is a need for thorough documentation of subjects covered by R5. Especially the unpacking often remains undocumented, which hinders replicability. Regarding our related observations on the impact of the quality over quantity credo in literature (cf., Section IV-E), we argue that there are many step stones such as missing firmware and content deduplication that must be documented to draw a better picture on representativeness and provide clean data for R3. Researchers may conduct more experiments that search for known vulnerabilities in firmware (R1). Finally, it is wise to improve the precision on all aspects of the device property measures in R6 – through documentation or artifact sharing.\n",
    "\n",
    "Thus, current practices in firmware vulnerability research meet our (arguably strict) requirements only partially: None of the 44 reviewed papers documents the subject of all 16 measures entirely. The results of Table II, Figure 4, and Figure 5 show that there is currently no common ground on sound firmware corpus creation and documentation in research. Missing meta data, incomplete documentation, and inflated corpus sizes blur visions on representativeness and hinder replicability.\n",
    "\n",
    "Overall, we found that there is currently no common ground on corpus creation and documentation in research. Also, we see that otherwise excellent work may fall into the trap of the methodological and practical challenges we discussed in Section II; impeding replicability and representativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736bf7d2-a4f5-4241-a709-b9a87477512f",
   "metadata": {},
   "source": [
    "### Figure 5: Aggregates the results of all collected data points for the associated measures in Table II. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728edf6c-1ed2-4a39-931a-1bd44b684e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_five(df_absolute: pd.DataFrame):\n",
    "    df_req_stats: pd.DataFrame = df_absolute.copy()\n",
    "\n",
    "    # this is from the matrix above table 2 that associates measures with requirements\n",
    "    df_req_stats[\"Requirements\"] = [\n",
    "        [\"R6\"],\n",
    "        [\"R3\", \"R6\"],\n",
    "        [\"R3\", \"R5\"],\n",
    "        [\"R5\"],\n",
    "        [\"R5\"],\n",
    "        [\"R5\"],\n",
    "        [\"R1\"],\n",
    "        [\"R2\", \"R4\"],\n",
    "        [\"R2\", \"R4\"],\n",
    "        [\"R4\"],\n",
    "        [\"R4\"],\n",
    "        [\"R2\", \"R4\", \"R6\"],\n",
    "        [\"R2\", \"R4\", \"R6\"],\n",
    "        [\"R2\", \"R4\", \"R6\"],\n",
    "        [\"R2\", \"R4\", \"R6\"],\n",
    "        [\"R2\", \"R4\", \"R6\"],\n",
    "    ]\n",
    "\n",
    "    df_req_stats = df_req_stats.explode(\"Requirements\").groupby([\"Requirements\"]).sum(numeric_only=True)\n",
    "    df_req_stats = df_req_stats.div(df_req_stats[\"Total Applicable\"], axis=0)\n",
    "    df_req_stats.drop(columns=[\"Total Applicable\"], inplace=True)\n",
    "    df_req_stats.rename(\n",
    "        columns={\"Yes\": \"Documented\", \"Unclear\": \"Partially Documented\", \"No\": \"Undocumented\"}, inplace=True\n",
    "    )\n",
    "    df_req_stats.reset_index(inplace=True)\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    ax = df_req_stats[::-1].plot(\n",
    "        kind=\"barh\",\n",
    "        stacked=True,\n",
    "        x=\"Requirements\",\n",
    "        grid=True,\n",
    "        color=[palette[0], palette[1], palette[-3]],\n",
    "        figsize=(8, 5),\n",
    "        legend=False,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.set_xticks(np.arange(0.0, 1.1, 0.1))\n",
    "    ax.set_xlabel(\"Fraction\")\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.legend(ncols=3, bbox_to_anchor=(0.93, 1.15))\n",
    "    ax.set_yticklabels(\n",
    "        [\n",
    "            \"R6) Heterogeneity\",\n",
    "            \"R5) Documentation\",\n",
    "            \"R4) Rich Meta Data\",\n",
    "            \"R3) Clean Data\",\n",
    "            \"R2) Relevance\",\n",
    "            \"R1) Ground Truth\",\n",
    "        ]\n",
    "    )\n",
    "    plt.savefig(FIGURE_DEST / \"f5_requirement_score_literature_tricolor.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_figure_five(df_absolute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e36408-c053-46c9-a68c-7ead2dc411b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Is there any trend regarding the raising awareness for replicability in the firmware security community?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e82c9b-10fe-4238-801a-7f0f4b65b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch_trend_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    raw_per_year_mean = {\"Year\": [], \"Documented\": [], \"Documented + Partially Documented\": []}\n",
    "    for year, group in df.groupby(\"Year\"):\n",
    "\n",
    "        raw_stats = {\n",
    "            \"Measure\": [],\n",
    "            \"Total Applicable\": [],\n",
    "            \"Yes\": [],\n",
    "            \"Unclear\": [],\n",
    "            \"No\": [],\n",
    "        }\n",
    "\n",
    "        for name, column in [t for t in group.items()][6:]:\n",
    "            total = column.count()\n",
    "            n = column[column.str.count(pat=\"N\") > 0].count()\n",
    "            u = column[column.str.count(pat=\"U\") > 0].count()\n",
    "            y = total - n - u\n",
    "\n",
    "            raw_stats[\"Measure\"] += [name]\n",
    "            raw_stats[\"Total Applicable\"] += [total]\n",
    "            raw_stats[\"Yes\"] += [y]\n",
    "            raw_stats[\"No\"] += [n]\n",
    "            raw_stats[\"Unclear\"] += [u]\n",
    "\n",
    "        tmp_abs = pd.DataFrame(raw_stats)\n",
    "\n",
    "        # relative stats\n",
    "        tmp_rel = tmp_abs.copy()\n",
    "        tmp_rel.drop(columns=[\"Total Applicable\"], inplace=True)\n",
    "        for col in [\"Yes\", \"No\", \"Unclear\"]:\n",
    "            tmp_rel[col] /= tmp_abs[\"Total Applicable\"]\n",
    "        raw_per_year_mean[\"Year\"] += [year]\n",
    "        raw_per_year_mean[\"Documented\"] += [tmp_rel[\"Yes\"].mean().round(2)]\n",
    "        raw_per_year_mean[\"Documented + Partially Documented\"] += [\n",
    "            (tmp_rel[\"Yes\"] + tmp_rel[\"Unclear\"]).mean().round(2)\n",
    "        ]\n",
    "    return pd.DataFrame(raw_per_year_mean)\n",
    "\n",
    "\n",
    "print(sketch_trend_analysis(df).to_markdown())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
